{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt-slQdlvmDK",
        "outputId": "14fe475f-b0b3-4082-ed7d-1acf32bd70a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üéÆ GPU Configuration\n",
            "================================================================================\n",
            "‚úÖ GPU detected: Tesla T4\n",
            "‚úÖ CUDA Version: 12.6\n",
            "‚úÖ GPU Memory: 15.83 GB\n",
            "‚úÖ Number of GPUs: 1\n",
            "‚úÖ cuDNN autotuner enabled\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üì• Downloading CIC-IoT-2023 Dataset from Kaggle...\n",
            "================================================================================\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/akashdogra/cic-iot-2023?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.77G/2.77G [00:28<00:00, 106MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset downloaded to: /root/.cache/kagglehub/datasets/akashdogra/cic-iot-2023/versions/1\n",
            "üìÇ Found 169 CSV files.\n",
            "\n",
            "üìä Dataset Split:\n",
            "   Training:   101 files\n",
            "   Validation: 34 files\n",
            "   Testing:    34 files\n",
            "\n",
            "================================================================================\n",
            "üè∑Ô∏è  Fitting Label Encoder...\n",
            "================================================================================\n",
            "Processing batch 1/21\n",
            "Processing batch 2/21\n",
            "Processing batch 3/21\n",
            "Processing batch 4/21\n",
            "Processing batch 5/21\n",
            "Processing batch 6/21\n",
            "Processing batch 7/21\n",
            "Processing batch 8/21\n",
            "Processing batch 9/21\n",
            "Processing batch 10/21\n",
            "Processing batch 11/21\n",
            "Processing batch 12/21\n",
            "Processing batch 13/21\n",
            "Processing batch 14/21\n",
            "Processing batch 15/21\n",
            "Processing batch 16/21\n",
            "Processing batch 17/21\n",
            "Processing batch 18/21\n",
            "Processing batch 19/21\n",
            "Processing batch 20/21\n",
            "Processing batch 21/21\n",
            "‚úÖ LabelEncoder fitted with 34 classes\n",
            "\n",
            "================================================================================\n",
            "üèóÔ∏è  Fitting Scaler & PCA...\n",
            "================================================================================\n",
            "PCA will use 30 components (dataset has 46 features)\n",
            "Pass 1: Fitting Scaler...\n",
            "  Scaler batch 1/21\n",
            "  Scaler batch 2/21\n",
            "  Scaler batch 3/21\n",
            "  Scaler batch 4/21\n",
            "  Scaler batch 5/21\n",
            "  Scaler batch 6/21\n",
            "  Scaler batch 7/21\n",
            "  Scaler batch 8/21\n",
            "  Scaler batch 9/21\n",
            "  Scaler batch 10/21\n",
            "  Scaler batch 11/21\n",
            "  Scaler batch 12/21\n",
            "  Scaler batch 13/21\n",
            "  Scaler batch 14/21\n",
            "  Scaler batch 15/21\n",
            "  Scaler batch 16/21\n",
            "  Scaler batch 17/21\n",
            "  Scaler batch 18/21\n",
            "  Scaler batch 19/21\n",
            "  Scaler batch 20/21\n",
            "  Scaler batch 21/21\n",
            "‚úÖ Scaler fitted\n",
            "\n",
            "Pass 2: Fitting PCA...\n",
            "  PCA batch 1/21\n",
            "  PCA batch 2/21\n",
            "  PCA batch 3/21\n",
            "  PCA batch 4/21\n",
            "  PCA batch 5/21\n",
            "  PCA batch 6/21\n",
            "  PCA batch 7/21\n",
            "  PCA batch 8/21\n",
            "  PCA batch 9/21\n",
            "  PCA batch 10/21\n",
            "  PCA batch 11/21\n",
            "  PCA batch 12/21\n",
            "  PCA batch 13/21\n",
            "  PCA batch 14/21\n",
            "  PCA batch 15/21\n",
            "  PCA batch 16/21\n",
            "  PCA batch 17/21\n",
            "  PCA batch 18/21\n",
            "  PCA batch 19/21\n",
            "  PCA batch 20/21\n",
            "  PCA batch 21/21\n",
            "‚úÖ PCA fitted with 30 components\n",
            "\n",
            "================================================================================\n",
            "üèóÔ∏è  Building Autoencoder Model...\n",
            "================================================================================\n",
            "\n",
            "üìä Model Architecture:\n",
            "   Input Size:      30\n",
            "   Bottleneck Size: 32\n",
            "   Output Classes:  34\n",
            "\n",
            "üìä Parameter Count:\n",
            "   Encoder:     14,688\n",
            "   Decoder:     14,686\n",
            "   Classifier:  4,322\n",
            "   Total:       33,696\n",
            "\n",
            "================================================================================\n",
            "üéØ Starting Training...\n",
            "================================================================================\n",
            "\n",
            "Training Configuration:\n",
            "   Epochs: 30\n",
            "   Files per epoch: 3\n",
            "   Loss weight (alpha): 0.5\n",
            "   Optimizer: Adam (lr=0.001)\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5631 (Recon: 1.5904, Class: 3.5358)\n",
            "Val Loss: 2.6038 (Recon: 1.6813, Class: 3.5263) | Val Acc: 0.0046\n",
            "‚úÖ Best model saved! Val Acc: 0.0046\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5898 (Recon: 1.6728, Class: 3.5068)\n",
            "Val Loss: 2.5995 (Recon: 1.6807, Class: 3.5182) | Val Acc: 0.0113\n",
            "‚úÖ Best model saved! Val Acc: 0.0113\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5376 (Recon: 1.5964, Class: 3.4788)\n",
            "Val Loss: 2.5946 (Recon: 1.6797, Class: 3.5095) | Val Acc: 0.0188\n",
            "‚úÖ Best model saved! Val Acc: 0.0188\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5888 (Recon: 1.7198, Class: 3.4578)\n",
            "Val Loss: 2.5893 (Recon: 1.6782, Class: 3.5004) | Val Acc: 0.0572\n",
            "‚úÖ Best model saved! Val Acc: 0.0572\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5640 (Recon: 1.6966, Class: 3.4314)\n",
            "Val Loss: 2.5833 (Recon: 1.6763, Class: 3.4903) | Val Acc: 0.0572\n",
            "‚úÖ Best model saved! Val Acc: 0.0572\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4757 (Recon: 1.5504, Class: 3.4009)\n",
            "Val Loss: 2.5760 (Recon: 1.6738, Class: 3.4783) | Val Acc: 0.0572\n",
            "\n",
            "================================================================================\n",
            "EPOCH 7/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4409 (Recon: 1.5112, Class: 3.3707)\n",
            "Val Loss: 2.5676 (Recon: 1.6706, Class: 3.4646) | Val Acc: 0.0584\n",
            "‚úÖ Best model saved! Val Acc: 0.0584\n",
            "\n",
            "================================================================================\n",
            "EPOCH 8/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4308 (Recon: 1.5197, Class: 3.3419)\n",
            "Val Loss: 2.5576 (Recon: 1.6668, Class: 3.4484) | Val Acc: 0.0584\n",
            "‚úÖ Best model saved! Val Acc: 0.0584\n",
            "\n",
            "================================================================================\n",
            "EPOCH 9/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5941 (Recon: 1.8740, Class: 3.3143)\n",
            "Val Loss: 2.5458 (Recon: 1.6623, Class: 3.4293) | Val Acc: 0.0572\n",
            "\n",
            "================================================================================\n",
            "EPOCH 10/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.7058 (Recon: 2.1149, Class: 3.2967)\n",
            "Val Loss: 2.5327 (Recon: 1.6573, Class: 3.4082) | Val Acc: 0.0751\n",
            "‚úÖ Best model saved! Val Acc: 0.0751\n",
            "\n",
            "================================================================================\n",
            "EPOCH 11/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4320 (Recon: 1.6199, Class: 3.2441)\n",
            "Val Loss: 2.5172 (Recon: 1.6513, Class: 3.3830) | Val Acc: 0.1679\n",
            "‚úÖ Best model saved! Val Acc: 0.1679\n",
            "\n",
            "================================================================================\n",
            "EPOCH 12/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.5080 (Recon: 1.8057, Class: 3.2102)\n",
            "Val Loss: 2.4994 (Recon: 1.6445, Class: 3.3544) | Val Acc: 0.1704\n",
            "‚úÖ Best model saved! Val Acc: 0.1704\n",
            "\n",
            "================================================================================\n",
            "EPOCH 13/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4616 (Recon: 1.7554, Class: 3.1677)\n",
            "Val Loss: 2.4794 (Recon: 1.6370, Class: 3.3218) | Val Acc: 0.1772\n",
            "‚úÖ Best model saved! Val Acc: 0.1772\n",
            "\n",
            "================================================================================\n",
            "EPOCH 14/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.2933 (Recon: 1.4806, Class: 3.1060)\n",
            "Val Loss: 2.4561 (Recon: 1.6282, Class: 3.2841) | Val Acc: 0.1935\n",
            "‚úÖ Best model saved! Val Acc: 0.1935\n",
            "\n",
            "================================================================================\n",
            "EPOCH 15/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.4218 (Recon: 1.7786, Class: 3.0650)\n",
            "Val Loss: 2.4304 (Recon: 1.6188, Class: 3.2420) | Val Acc: 0.3778\n",
            "‚úÖ Best model saved! Val Acc: 0.3778\n",
            "\n",
            "================================================================================\n",
            "EPOCH 16/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.2738 (Recon: 1.5331, Class: 3.0145)\n",
            "Val Loss: 2.4014 (Recon: 1.6083, Class: 3.1945) | Val Acc: 0.4799\n",
            "‚úÖ Best model saved! Val Acc: 0.4799\n",
            "\n",
            "================================================================================\n",
            "EPOCH 17/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.1803 (Recon: 1.4158, Class: 2.9448)\n",
            "Val Loss: 2.3684 (Recon: 1.5969, Class: 3.1400) | Val Acc: 0.4866\n",
            "‚úÖ Best model saved! Val Acc: 0.4866\n",
            "\n",
            "================================================================================\n",
            "EPOCH 18/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.6520 (Recon: 2.3891, Class: 2.9149)\n",
            "Val Loss: 2.3347 (Recon: 1.5855, Class: 3.0839) | Val Acc: 0.5594\n",
            "‚úÖ Best model saved! Val Acc: 0.5594\n",
            "\n",
            "================================================================================\n",
            "EPOCH 19/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.0949 (Recon: 1.3679, Class: 2.8219)\n",
            "Val Loss: 2.2955 (Recon: 1.5732, Class: 3.0178) | Val Acc: 0.5724\n",
            "‚úÖ Best model saved! Val Acc: 0.5724\n",
            "\n",
            "================================================================================\n",
            "EPOCH 20/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.1005 (Recon: 1.4375, Class: 2.7635)\n",
            "Val Loss: 2.2532 (Recon: 1.5608, Class: 2.9457) | Val Acc: 0.5789\n",
            "‚úÖ Best model saved! Val Acc: 0.5789\n",
            "\n",
            "================================================================================\n",
            "EPOCH 21/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.0253 (Recon: 1.3585, Class: 2.6920)\n",
            "Val Loss: 2.2078 (Recon: 1.5485, Class: 2.8672) | Val Acc: 0.5802\n",
            "‚úÖ Best model saved! Val Acc: 0.5802\n",
            "\n",
            "================================================================================\n",
            "EPOCH 22/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.0035 (Recon: 1.3825, Class: 2.6245)\n",
            "Val Loss: 2.1596 (Recon: 1.5365, Class: 2.7828) | Val Acc: 0.5804\n",
            "‚úÖ Best model saved! Val Acc: 0.5804\n",
            "\n",
            "================================================================================\n",
            "EPOCH 23/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.0356 (Recon: 1.5076, Class: 2.5637)\n",
            "Val Loss: 2.1094 (Recon: 1.5252, Class: 2.6936) | Val Acc: 0.5805\n",
            "‚úÖ Best model saved! Val Acc: 0.5805\n",
            "\n",
            "================================================================================\n",
            "EPOCH 24/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.0138 (Recon: 1.5372, Class: 2.4903)\n",
            "Val Loss: 2.0570 (Recon: 1.5146, Class: 2.5994) | Val Acc: 0.5806\n",
            "‚úÖ Best model saved! Val Acc: 0.5806\n",
            "\n",
            "================================================================================\n",
            "EPOCH 25/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 1.9849 (Recon: 1.5521, Class: 2.4177)\n",
            "Val Loss: 2.0031 (Recon: 1.5050, Class: 2.5013) | Val Acc: 0.5808\n",
            "‚úÖ Best model saved! Val Acc: 0.5808\n",
            "\n",
            "================================================================================\n",
            "EPOCH 26/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 1.9352 (Recon: 1.5316, Class: 2.3387)\n",
            "Val Loss: 1.9477 (Recon: 1.4962, Class: 2.3992) | Val Acc: 0.5808\n",
            "‚úÖ Best model saved! Val Acc: 0.5808\n",
            "\n",
            "================================================================================\n",
            "EPOCH 27/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 1.9106 (Recon: 1.5604, Class: 2.2608)\n",
            "Val Loss: 1.8918 (Recon: 1.4884, Class: 2.2951) | Val Acc: 0.5813\n",
            "‚úÖ Best model saved! Val Acc: 0.5813\n",
            "\n",
            "================================================================================\n",
            "EPOCH 28/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 1.7500 (Recon: 1.3298, Class: 2.1702)\n",
            "Val Loss: 1.8362 (Recon: 1.4812, Class: 2.1913) | Val Acc: 0.5817\n",
            "‚úÖ Best model saved! Val Acc: 0.5817\n",
            "\n",
            "================================================================================\n",
            "EPOCH 29/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 2.1253 (Recon: 2.1179, Class: 2.1327)\n",
            "Val Loss: 1.7871 (Recon: 1.4751, Class: 2.0990) | Val Acc: 0.5818\n",
            "‚úÖ Best model saved! Val Acc: 0.5818\n",
            "\n",
            "================================================================================\n",
            "EPOCH 30/30\n",
            "================================================================================\n",
            "Training on 3 files\n",
            "Train Loss: 1.9881 (Recon: 1.9408, Class: 2.0354)\n",
            "Val Loss: 1.7355 (Recon: 1.4694, Class: 2.0015) | Val Acc: 0.5819\n",
            "‚úÖ Best model saved! Val Acc: 0.5819\n",
            "\n",
            "‚úÖ Training Complete!\n",
            "Best Validation Accuracy: 0.5819\n",
            "\n",
            "================================================================================\n",
            "üìà Final Evaluation on Test Set\n",
            "================================================================================\n",
            "Test batch 1/7\n",
            "Test batch 2/7\n",
            "Test batch 3/7\n",
            "Test batch 4/7\n",
            "Test batch 5/7\n",
            "Test batch 6/7\n",
            "Test batch 7/7\n",
            "\n",
            "üìä Test Set Performance:\n",
            "   Accuracy:  0.5808\n",
            "   Precision: 0.5354\n",
            "   Recall:    0.5808\n",
            "   F1-Score:  0.4905\n",
            "\n",
            "================================================================================\n",
            "üìä Generating Confusion Matrix...\n",
            "================================================================================\n",
            "‚úÖ Confusion matrix saved as 'autoencoder_confusion_matrix.png'\n",
            "\n",
            "================================================================================\n",
            "üìà Generating Training History Plots...\n",
            "================================================================================\n",
            "‚úÖ Training history saved as 'training_history.png'\n",
            "\n",
            "================================================================================\n",
            "üíæ Saving Model and Artifacts\n",
            "================================================================================\n",
            "‚úÖ Saved: preprocessing.pkl\n",
            "‚úÖ Saved: autoencoder_metadata.json\n",
            "‚úÖ Saved: autoencoder_summary.txt\n",
            "\n",
            "================================================================================\n",
            "üìÅ Generated Files:\n",
            "================================================================================\n",
            "‚úÖ autoencoder_model.pth                               0.42 MB\n",
            "‚úÖ preprocessing.pkl                                   0.02 MB\n",
            "‚úÖ autoencoder_metadata.json                           0.01 MB\n",
            "‚úÖ autoencoder_summary.txt                             0.00 MB\n",
            "‚úÖ autoencoder_confusion_matrix.png                    1.21 MB\n",
            "‚úÖ training_history.png                                0.59 MB\n",
            "================================================================================\n",
            "üìä Total Size: 2.24 MB\n",
            "\n",
            "================================================================================\n",
            "üéâ AUTOENCODER TRAINING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Final Test Accuracy: 0.5808\n",
            "Model Parameters: 33,696\n",
            "\n",
            "All files have been saved and are ready for download.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Autoencoder for IoT Intrusion Detection - PyTorch Implementation\n",
        "Dense Layer Architecture\n",
        "60-20-20 Train-Val-Test Split\n",
        "Max 5 files loaded at once\n",
        "GPU Accelerated\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kagglehub\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================================================\n",
        "# üéÆ GPU CONFIGURATION\n",
        "# ==========================================================\n",
        "\n",
        "def setup_gpu():\n",
        "    \"\"\"Configure PyTorch to use GPU efficiently\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üéÆ GPU Configuration\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        print(\"‚úÖ cuDNN autotuner enabled\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"‚ö†Ô∏è  No GPU detected, running on CPU\")\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    return device\n",
        "\n",
        "device = setup_gpu()\n",
        "\n",
        "# ==========================================================\n",
        "# üßπ HELPER FUNCTIONS\n",
        "# ==========================================================\n",
        "\n",
        "def load_and_clean(path, label_col=None):\n",
        "    \"\"\"Load CSV and separate features from labels\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    if label_col is None:\n",
        "        label_col = \"Label\" if \"Label\" in df.columns else df.columns[-1]\n",
        "\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def encode_objects(X):\n",
        "    \"\"\"Encode categorical columns and convert to numpy array\"\"\"\n",
        "    for col in X.select_dtypes(include=[\"object\"]).columns:\n",
        "        X[col] = LabelEncoder().fit_transform(X[col])\n",
        "    return X.values\n",
        "\n",
        "\n",
        "def process_files_generator(file_list, scaler, pca, label_encoder, batch_size=5):\n",
        "    \"\"\"Generator that yields batches of processed data without storing all in memory\"\"\"\n",
        "    for i in range(0, len(file_list), batch_size):\n",
        "        batch_files = file_list[i:i+batch_size]\n",
        "\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        for f in batch_files:\n",
        "            try:\n",
        "                X, y = load_and_clean(f)\n",
        "                X = encode_objects(X)\n",
        "\n",
        "                X_scaled = scaler.transform(X)\n",
        "                X_reduced = pca.transform(X_scaled)\n",
        "\n",
        "                X_batch.append(X_reduced)\n",
        "                y_batch.append(label_encoder.transform(y.astype(str)))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {f}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if X_batch:\n",
        "            X_combined = np.vstack(X_batch)\n",
        "            y_combined = np.hstack(y_batch)\n",
        "\n",
        "            del X_batch, y_batch\n",
        "            gc.collect()\n",
        "\n",
        "            yield X_combined, y_combined\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# üèóÔ∏è AUTOENCODER MODEL\n",
        "# ==========================================================\n",
        "\n",
        "class DenseAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder with Dense (Fully Connected) Layers\n",
        "    Encoder: input -> 128 -> 64 -> 32 (bottleneck)\n",
        "    Decoder: 32 -> 64 -> 128 -> output\n",
        "    Classifier: 32 (bottleneck) -> num_classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, num_classes, bottleneck_size=32):\n",
        "        super(DenseAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, bottleneck_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, input_size),\n",
        "            nn.Sigmoid()  # Output in [0, 1] range\n",
        "        )\n",
        "\n",
        "        # Classifier (uses bottleneck features)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(bottleneck_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_bottleneck=False):\n",
        "        # Encode\n",
        "        bottleneck = self.encoder(x)\n",
        "\n",
        "        # Decode\n",
        "        reconstructed = self.decoder(bottleneck)\n",
        "\n",
        "        # Classify\n",
        "        classification = self.classifier(bottleneck)\n",
        "\n",
        "        if return_bottleneck:\n",
        "            return reconstructed, classification, bottleneck\n",
        "        return reconstructed, classification\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Get bottleneck representation\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# üèãÔ∏è TRAINING FUNCTIONS\n",
        "# ==========================================================\n",
        "\n",
        "def train_epoch(model, data_generator, optimizer, device, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Train for one epoch with combined loss\n",
        "    alpha: weight between reconstruction and classification loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_recon_loss = 0\n",
        "    total_class_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for X_batch, y_batch in data_generator:\n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.FloatTensor(X_batch).to(device)\n",
        "        y_tensor = torch.LongTensor(y_batch).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed, classification = model(X_tensor)\n",
        "\n",
        "        # Reconstruction loss (autoencoder)\n",
        "        recon_loss = reconstruction_criterion(reconstructed, X_tensor)\n",
        "\n",
        "        # Classification loss\n",
        "        class_loss = classification_criterion(classification, y_tensor)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = alpha * recon_loss + (1 - alpha) * class_loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y_batch)\n",
        "        total_recon_loss += recon_loss.item() * len(y_batch)\n",
        "        total_class_loss += class_loss.item() * len(y_batch)\n",
        "        total_samples += len(y_batch)\n",
        "\n",
        "        # Free memory\n",
        "        del X_tensor, y_tensor, reconstructed, classification\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_recon = total_recon_loss / total_samples\n",
        "    avg_class = total_class_loss / total_samples\n",
        "\n",
        "    return avg_loss, avg_recon, avg_class\n",
        "\n",
        "\n",
        "def evaluate(model, data_generator, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_recon_loss = 0\n",
        "    total_class_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_generator:\n",
        "            X_tensor = torch.FloatTensor(X_batch).to(device)\n",
        "            y_tensor = torch.LongTensor(y_batch).to(device)\n",
        "\n",
        "            reconstructed, classification = model(X_tensor)\n",
        "\n",
        "            recon_loss = reconstruction_criterion(reconstructed, X_tensor)\n",
        "            class_loss = classification_criterion(classification, y_tensor)\n",
        "            loss = 0.5 * recon_loss + 0.5 * class_loss\n",
        "\n",
        "            _, predicted = torch.max(classification, 1)\n",
        "            correct += (predicted == y_tensor).sum().item()\n",
        "\n",
        "            total_loss += loss.item() * len(y_batch)\n",
        "            total_recon_loss += recon_loss.item() * len(y_batch)\n",
        "            total_class_loss += class_loss.item() * len(y_batch)\n",
        "            total_samples += len(y_batch)\n",
        "\n",
        "            del X_tensor, y_tensor, reconstructed, classification\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = correct / total_samples\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_recon = total_recon_loss / total_samples\n",
        "    avg_class = total_class_loss / total_samples\n",
        "\n",
        "    return avg_loss, avg_recon, avg_class, accuracy\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# üìÇ DOWNLOAD & SPLIT DATASET\n",
        "# ==========================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üì• Downloading CIC-IoT-2023 Dataset from Kaggle...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "dataset_dir = kagglehub.dataset_download(\"akashdogra/cic-iot-2023\")\n",
        "print(f\"‚úÖ Dataset downloaded to: {dataset_dir}\")\n",
        "\n",
        "csv_files = sorted([\n",
        "    os.path.join(dataset_dir, f)\n",
        "    for f in os.listdir(dataset_dir)\n",
        "    if f.endswith(\".csv\")\n",
        "])\n",
        "\n",
        "print(f\"üìÇ Found {len(csv_files)} CSV files.\")\n",
        "\n",
        "# 60-20-20 split\n",
        "n_files = len(csv_files)\n",
        "train_idx = int(n_files * 0.60)\n",
        "val_idx = int(n_files * 0.80)\n",
        "\n",
        "train_files = csv_files[:train_idx]\n",
        "val_files = csv_files[train_idx:val_idx]\n",
        "test_files = csv_files[val_idx:]\n",
        "\n",
        "print(f\"\\nüìä Dataset Split:\")\n",
        "print(f\"   Training:   {len(train_files)} files\")\n",
        "print(f\"   Validation: {len(val_files)} files\")\n",
        "print(f\"   Testing:    {len(test_files)} files\")\n",
        "\n",
        "# ==========================================================\n",
        "# üè∑Ô∏è FIT LABEL ENCODER\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üè∑Ô∏è  Fitting Label Encoder...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_labels = []\n",
        "max_batch = 5\n",
        "\n",
        "for i in range(0, len(train_files), max_batch):\n",
        "    batch_files = train_files[i:i+max_batch]\n",
        "    print(f\"Processing batch {i//max_batch + 1}/{(len(train_files)-1)//max_batch + 1}\")\n",
        "\n",
        "    for f in batch_files:\n",
        "        _, y = load_and_clean(f)\n",
        "        all_labels.extend(list(y.astype(str)))\n",
        "\n",
        "    if i % (max_batch * 4) == 0:\n",
        "        gc.collect()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "del all_labels\n",
        "gc.collect()\n",
        "\n",
        "print(f\"‚úÖ LabelEncoder fitted with {len(label_encoder.classes_)} classes\")\n",
        "\n",
        "# ==========================================================\n",
        "# üèóÔ∏è FIT SCALER & PCA\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üèóÔ∏è  Fitting Scaler & PCA...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "sample_X, _ = load_and_clean(train_files[0])\n",
        "sample_X = encode_objects(sample_X)\n",
        "n_features = sample_X.shape[1]\n",
        "n_components = min(30, n_features)\n",
        "del sample_X\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PCA will use {n_components} components (dataset has {n_features} features)\")\n",
        "\n",
        "pca = IncrementalPCA(n_components=n_components)\n",
        "\n",
        "# Pass 1: Fit Scaler\n",
        "print(\"Pass 1: Fitting Scaler...\")\n",
        "for i in range(0, len(train_files), max_batch):\n",
        "    batch_files = train_files[i:i+max_batch]\n",
        "    print(f\"  Scaler batch {i//max_batch + 1}/{(len(train_files)-1)//max_batch + 1}\")\n",
        "\n",
        "    for f in batch_files:\n",
        "        X, _ = load_and_clean(f)\n",
        "        X = encode_objects(X)\n",
        "        scaler.partial_fit(X)\n",
        "        del X\n",
        "        gc.collect()\n",
        "\n",
        "print(\"‚úÖ Scaler fitted\")\n",
        "\n",
        "# Pass 2: Fit PCA\n",
        "print(\"\\nPass 2: Fitting PCA...\")\n",
        "for i in range(0, len(train_files), max_batch):\n",
        "    batch_files = train_files[i:i+max_batch]\n",
        "    print(f\"  PCA batch {i//max_batch + 1}/{(len(train_files)-1)//max_batch + 1}\")\n",
        "\n",
        "    for f in batch_files:\n",
        "        X, _ = load_and_clean(f)\n",
        "        X = encode_objects(X)\n",
        "        X_scaled = scaler.transform(X)\n",
        "        pca.partial_fit(X_scaled)\n",
        "        del X, X_scaled\n",
        "        gc.collect()\n",
        "\n",
        "print(f\"‚úÖ PCA fitted with {pca.n_components_} components\")\n",
        "gc.collect()\n",
        "\n",
        "# ==========================================================\n",
        "# üèóÔ∏è BUILD AUTOENCODER MODEL\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üèóÔ∏è  Building Autoencoder Model...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "bottleneck_size = 32\n",
        "\n",
        "model = DenseAutoencoder(\n",
        "    input_size=n_components,\n",
        "    num_classes=n_classes,\n",
        "    bottleneck_size=bottleneck_size\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
        "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
        "classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
        "\n",
        "print(f\"\\nüìä Model Architecture:\")\n",
        "print(f\"   Input Size:      {n_components}\")\n",
        "print(f\"   Bottleneck Size: {bottleneck_size}\")\n",
        "print(f\"   Output Classes:  {n_classes}\")\n",
        "print(f\"\\nüìä Parameter Count:\")\n",
        "print(f\"   Encoder:     {encoder_params:,}\")\n",
        "print(f\"   Decoder:     {decoder_params:,}\")\n",
        "print(f\"   Classifier:  {classifier_params:,}\")\n",
        "print(f\"   Total:       {total_params:,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ==========================================================\n",
        "# üéØ TRAINING\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ Starting Training...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "epochs = 30\n",
        "files_per_epoch = 3\n",
        "alpha = 0.5  # Weight between reconstruction and classification\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "patience = 5\n",
        "\n",
        "training_history = {\n",
        "    'train_loss': [],\n",
        "    'train_recon': [],\n",
        "    'train_class': [],\n",
        "    'val_loss': [],\n",
        "    'val_recon': [],\n",
        "    'val_class': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"   Epochs: {epochs}\")\n",
        "print(f\"   Files per epoch: {files_per_epoch}\")\n",
        "print(f\"   Loss weight (alpha): {alpha}\")\n",
        "print(f\"   Optimizer: Adam (lr=0.001)\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Select training files\n",
        "    start = (epoch * files_per_epoch) % len(train_files)\n",
        "    selected_files = train_files[start:start + files_per_epoch]\n",
        "\n",
        "    if len(selected_files) < files_per_epoch:\n",
        "        selected_files += train_files[:files_per_epoch - len(selected_files)]\n",
        "\n",
        "    print(f\"Training on {len(selected_files)} files\")\n",
        "\n",
        "    # Train\n",
        "    train_gen = process_files_generator(selected_files, scaler, pca, label_encoder, batch_size=files_per_epoch)\n",
        "    train_loss, train_recon, train_class = train_epoch(model, train_gen, optimizer, device, alpha)\n",
        "\n",
        "    # Validate\n",
        "    val_gen = process_files_generator(val_files[:5], scaler, pca, label_encoder, batch_size=5)\n",
        "    val_loss, val_recon, val_class, val_acc = evaluate(model, val_gen, device)\n",
        "\n",
        "    # Store history\n",
        "    training_history['train_loss'].append(train_loss)\n",
        "    training_history['train_recon'].append(train_recon)\n",
        "    training_history['train_class'].append(train_class)\n",
        "    training_history['val_loss'].append(val_loss)\n",
        "    training_history['val_recon'].append(val_recon)\n",
        "    training_history['val_class'].append(val_class)\n",
        "    training_history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, Class: {train_class:.4f})\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, Class: {val_class:.4f}) | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'input_size': n_components,\n",
        "            'num_classes': n_classes,\n",
        "            'bottleneck_size': bottleneck_size\n",
        "        }, 'autoencoder_model.pth')\n",
        "        print(f\"‚úÖ Best model saved! Val Acc: {val_acc:.4f}\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n‚úÖ Training Complete!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('autoencoder_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# ==========================================================\n",
        "# üìà FINAL EVALUATION ON TEST SET\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà Final Evaluation on Test Set\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model.eval()\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "\n",
        "test_gen = process_files_generator(test_files, scaler, pca, label_encoder, batch_size=5)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_num, (X_test, y_test) in enumerate(test_gen):\n",
        "        print(f\"Test batch {batch_num + 1}/{(len(test_files)-1)//5 + 1}\")\n",
        "\n",
        "        X_tensor = torch.FloatTensor(X_test).to(device)\n",
        "        _, classification = model(X_tensor)\n",
        "        _, predicted = torch.max(classification, 1)\n",
        "\n",
        "        y_true_all.extend(y_test)\n",
        "        y_pred_all.extend(predicted.cpu().numpy())\n",
        "\n",
        "        del X_tensor, classification\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "y_true_all = np.array(y_true_all)\n",
        "y_pred_all = np.array(y_pred_all)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true_all, y_pred_all)\n",
        "precision = precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nüìä Test Set Performance:\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# ==========================================================\n",
        "# üìä CONFUSION MATRIX\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Generating Confusion Matrix...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Autoencoder Confusion Matrix - IoT Intrusion Detection', fontsize=16, pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(rotation=0, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('autoencoder_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Confusion matrix saved as 'autoencoder_confusion_matrix.png'\")\n",
        "\n",
        "# ==========================================================\n",
        "# üìà TRAINING HISTORY PLOTS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà Generating Training History Plots...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Total Loss\n",
        "axes[0, 0].plot(training_history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0, 0].plot(training_history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0, 0].set_title('Total Loss Over Epochs')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Plot 2: Reconstruction Loss\n",
        "axes[0, 1].plot(training_history['train_recon'], label='Train Recon', marker='o')\n",
        "axes[0, 1].plot(training_history['val_recon'], label='Val Recon', marker='s')\n",
        "axes[0, 1].set_title('Reconstruction Loss Over Epochs')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('MSE Loss')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# Plot 3: Classification Loss\n",
        "axes[1, 0].plot(training_history['train_class'], label='Train Class', marker='o')\n",
        "axes[1, 0].plot(training_history['val_class'], label='Val Class', marker='s')\n",
        "axes[1, 0].set_title('Classification Loss Over Epochs')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Cross-Entropy Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# Plot 4: Validation Accuracy\n",
        "axes[1, 1].plot(training_history['val_acc'], label='Val Accuracy', marker='s', color='green')\n",
        "axes[1, 1].set_title('Validation Accuracy Over Epochs')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Training history saved as 'training_history.png'\")\n",
        "\n",
        "# ==========================================================\n",
        "# üíæ SAVE MODEL AND ARTIFACTS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üíæ Saving Model and Artifacts\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save preprocessing objects\n",
        "preprocessing_objects = {\n",
        "    'scaler': scaler,\n",
        "    'pca': pca,\n",
        "    'label_encoder': label_encoder\n",
        "}\n",
        "\n",
        "with open('preprocessing.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_objects, f)\n",
        "print(\"‚úÖ Saved: preprocessing.pkl\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'n_classes': int(n_classes),\n",
        "    'n_features': int(n_features),\n",
        "    'n_components': int(n_components),\n",
        "    'bottleneck_size': int(bottleneck_size),\n",
        "    'total_params': int(total_params),\n",
        "    'encoder_params': int(encoder_params),\n",
        "    'decoder_params': int(decoder_params),\n",
        "    'classifier_params': int(classifier_params),\n",
        "    'test_accuracy': float(accuracy),\n",
        "    'test_precision': float(precision),\n",
        "    'test_recall': float(recall),\n",
        "    'test_f1': float(f1),\n",
        "    'best_val_acc': float(best_val_acc),\n",
        "    'classes': label_encoder.classes_.tolist(),\n",
        "    'training_history': {k: [float(v) for v in vals] for k, vals in training_history.items()}\n",
        "}\n",
        "\n",
        "with open('autoencoder_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "print(\"‚úÖ Saved: autoencoder_metadata.json\")\n",
        "\n",
        "# Create summary\n",
        "with open('autoencoder_summary.txt', 'w') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"AUTOENCODER MODEL SUMMARY\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"MODEL ARCHITECTURE:\\n\")\n",
        "    f.write(f\"  Input Size:      {n_components}\\n\")\n",
        "    f.write(f\"  Bottleneck Size: {bottleneck_size}\\n\")\n",
        "    f.write(f\"  Output Classes:  {n_classes}\\n\\n\")\n",
        "\n",
        "    f.write(\"PARAMETER COUNT:\\n\")\n",
        "    f.write(f\"  Encoder:     {encoder_params:,}\\n\")\n",
        "    f.write(f\"  Decoder:     {decoder_params:,}\\n\")\n",
        "    f.write(f\"  Classifier:  {classifier_params:,}\\n\")\n",
        "    f.write(f\"  Total:       {total_params:,}\\n\\n\")\n",
        "\n",
        "    f.write(\"TEST SET PERFORMANCE:\\n\")\n",
        "    f.write(f\"  Accuracy:  {accuracy:.4f}\\n\")\n",
        "    f.write(f\"  Precision: {precision:.4f}\\n\")\n",
        "    f.write(f\"  Recall:    {recall:.4f}\\n\")\n",
        "    f.write(f\"  F1-Score:  {f1:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"FILES GENERATED:\\n\")\n",
        "    f.write(\"  - autoencoder_model.pth (Model checkpoint)\\n\")\n",
        "    f.write(\"  - preprocessing.pkl (Scaler, PCA, Label Encoder)\\n\")\n",
        "    f.write(\"  - autoencoder_metadata.json (Model specifications)\\n\")\n",
        "    f.write(\"  - autoencoder_confusion_matrix.png (Confusion matrix)\\n\")\n",
        "    f.write(\"  - training_history.png (Training curves)\\n\")\n",
        "\n",
        "print(\"‚úÖ Saved: autoencoder_summary.txt\")\n",
        "\n",
        "# List all files\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìÅ Generated Files:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "files_to_check = [\n",
        "    'autoencoder_model.pth',\n",
        "    'preprocessing.pkl',\n",
        "    'autoencoder_metadata.json',\n",
        "    'autoencoder_summary.txt',\n",
        "    'autoencoder_confusion_matrix.png',\n",
        "    'training_history.png'\n",
        "]\n",
        "\n",
        "total_size = 0\n",
        "for filename in files_to_check:\n",
        "    if os.path.exists(filename):\n",
        "        size = os.path.getsize(filename)\n",
        "        total_size += size\n",
        "        size_mb = size / (1024 * 1024)\n",
        "        print(f\"‚úÖ {filename:<45} {size_mb:>10.2f} MB\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìä Total Size: {total_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ AUTOENCODER TRAINING COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFinal Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Model Parameters: {total_params:,}\")\n",
        "print(\"\\nAll files have been saved and are ready for download.\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsqwddqlvttR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}