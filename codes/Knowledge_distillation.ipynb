{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Knowledge Distillation for IoT Intrusion Detection - Full File Processing\n",
        "- Reduced model sizes for memory efficiency\n",
        "- Process entire files at once (no chunking within files)\n",
        "- Handle all 169 files properly\n",
        "- Stream one file at a time to avoid RAM overflow\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kagglehub\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================================================\n",
        "# üéÆ GPU CONFIGURATION\n",
        "# ==========================================================\n",
        "\n",
        "def setup_gpu():\n",
        "    \"\"\"Configure PyTorch to use GPU efficiently\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üéÆ GPU Configuration\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        print(\"‚úÖ cuDNN autotuner enabled\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"‚ö†Ô∏è  No GPU detected, running on CPU\")\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    return device\n",
        "\n",
        "device = setup_gpu()\n",
        "\n",
        "# ==========================================================\n",
        "# üßπ MEMORY MANAGEMENT\n",
        "# ==========================================================\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Aggressive memory cleanup\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def print_memory_stats():\n",
        "    \"\"\"Print RAM and GPU usage\"\"\"\n",
        "    try:\n",
        "        import psutil\n",
        "        process = psutil.Process()\n",
        "        ram_gb = process.memory_info().rss / 1e9\n",
        "        print(f\"üíæ RAM Usage: {ram_gb:.2f} GB\", end=\"\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_gb = torch.cuda.memory_allocated() / 1e9\n",
        "        print(f\" | GPU: {gpu_gb:.2f} GB\")\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "# ==========================================================\n",
        "# üßπ HELPER FUNCTIONS\n",
        "# ==========================================================\n",
        "\n",
        "def load_and_clean(path, label_col=None):\n",
        "    \"\"\"Load CSV and separate features from labels\"\"\"\n",
        "    df = pd.read_csv(path, low_memory=False)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    if label_col is None:\n",
        "        label_col = \"Label\" if \"Label\" in df.columns else df.columns[-1]\n",
        "\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def encode_objects(X):\n",
        "    \"\"\"Encode categorical columns and convert to numpy array\"\"\"\n",
        "    for col in X.select_dtypes(include=[\"object\"]).columns:\n",
        "        try:\n",
        "            X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "        except:\n",
        "            X[col] = 0\n",
        "    return X.values.astype(np.float32)\n",
        "\n",
        "def load_and_process_file(filepath, scaler, pca, label_encoder):\n",
        "    \"\"\"Load and process a single file completely\"\"\"\n",
        "    try:\n",
        "        X, y = load_and_clean(filepath)\n",
        "        X = encode_objects(X)\n",
        "\n",
        "        X_scaled = scaler.transform(X)\n",
        "        X_reduced = pca.transform(X_scaled)\n",
        "        y_encoded = label_encoder.transform(y.astype(str))\n",
        "\n",
        "        del X, y, X_scaled\n",
        "        gc.collect()\n",
        "\n",
        "        return X_reduced, y_encoded\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {os.path.basename(filepath)}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# ==========================================================\n",
        "# üì¶ FULL FILE DATASET\n",
        "# ==========================================================\n",
        "\n",
        "class FullFileDataset(Dataset):\n",
        "    \"\"\"Dataset that holds entire file in memory\"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ==========================================================\n",
        "# üéì REDUCED PYTORCH MODELS\n",
        "# ==========================================================\n",
        "\n",
        "class TeacherLSTM(nn.Module):\n",
        "    \"\"\"Teacher Model - [128, 64] (Reduced from [256,128,64])\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.3):\n",
        "        super(TeacherLSTM, self).__init__()\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_sizes[1], 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, features)\n",
        "        out, _ = self.lstm1(x)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out, _ = self.lstm2(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        # Take last timestep\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        out = self.relu1(self.fc1(out))\n",
        "        out = self.relu2(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StudentLSTM(nn.Module):\n",
        "    \"\"\"Student Model - [32] (Reduced from [32,16])\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout=0.2):\n",
        "        super(StudentLSTM, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Take last timestep\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# ==========================================================\n",
        "# üéì KNOWLEDGE DISTILLATION LOSS\n",
        "# ==========================================================\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"Combined loss for knowledge distillation\"\"\"\n",
        "\n",
        "    def __init__(self, temperature=4.0, alpha=0.7):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Hard target loss\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Soft target loss\n",
        "        soft_student = torch.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        soft_teacher = torch.softmax(teacher_logits / self.temperature, dim=1)\n",
        "\n",
        "        soft_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# ==========================================================\n",
        "# üèãÔ∏è TRAINING FUNCTIONS (FULL FILE AT ONCE)\n",
        "# ==========================================================\n",
        "\n",
        "def train_on_file(model, filepath, scaler, pca, label_encoder, optimizer,\n",
        "                  criterion, device, batch_size=512, is_distillation=False,\n",
        "                  teacher_model=None):\n",
        "    \"\"\"Train on entire file at once\"\"\"\n",
        "\n",
        "    # Load and process entire file\n",
        "    X_file, y_file = load_and_process_file(filepath, scaler, pca, label_encoder)\n",
        "\n",
        "    if X_file is None:\n",
        "        return 0\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = FullFileDataset(X_file, y_file)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    model.train()\n",
        "    if teacher_model is not None:\n",
        "        teacher_model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch = X_batch.unsqueeze(1).to(device)  # Add sequence dimension\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        if is_distillation and teacher_model is not None:\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(X_batch)\n",
        "            loss = criterion(outputs, teacher_outputs, y_batch)\n",
        "        else:\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y_batch)\n",
        "        total_samples += len(y_batch)\n",
        "\n",
        "        del X_batch, y_batch, outputs\n",
        "        clear_memory()\n",
        "\n",
        "    # Clean up file data\n",
        "    del X_file, y_file, dataset, dataloader\n",
        "    clear_memory()\n",
        "\n",
        "    return total_loss / total_samples if total_samples > 0 else 0\n",
        "\n",
        "\n",
        "def evaluate_on_files(model, file_list, scaler, pca, label_encoder,\n",
        "                      criterion, device, batch_size=512):\n",
        "    \"\"\"Evaluate on multiple files\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for filepath in file_list:\n",
        "        X_file, y_file = load_and_process_file(filepath, scaler, pca, label_encoder)\n",
        "\n",
        "        if X_file is None:\n",
        "            continue\n",
        "\n",
        "        dataset = FullFileDataset(X_file, y_file)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in dataloader:\n",
        "                X_batch = X_batch.unsqueeze(1).to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == y_batch).sum().item()\n",
        "                total_loss += loss.item() * len(y_batch)\n",
        "                total_samples += len(y_batch)\n",
        "\n",
        "                del X_batch, y_batch, outputs\n",
        "                clear_memory()\n",
        "\n",
        "        del X_file, y_file, dataset, dataloader\n",
        "        clear_memory()\n",
        "\n",
        "    accuracy = correct / total_samples if total_samples > 0 else 0\n",
        "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ==========================================================\n",
        "# üìÇ DOWNLOAD & SPLIT DATASET (169 FILES)\n",
        "# ==========================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üì• Downloading CIC-IoT-2023 Dataset from Kaggle...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "dataset_dir = kagglehub.dataset_download(\"akashdogra/cic-iot-2023\")\n",
        "print(f\"‚úÖ Dataset downloaded to: {dataset_dir}\")\n",
        "\n",
        "csv_files = sorted([\n",
        "    os.path.join(dataset_dir, f)\n",
        "    for f in os.listdir(dataset_dir)\n",
        "    if f.endswith(\".csv\")\n",
        "])\n",
        "\n",
        "print(f\"üìÇ Found {len(csv_files)} CSV files.\")\n",
        "\n",
        "# 60-20-20 split\n",
        "n_files = len(csv_files)\n",
        "train_idx = int(n_files * 0.60)\n",
        "val_idx = int(n_files * 0.80)\n",
        "\n",
        "train_files = csv_files[:train_idx]\n",
        "val_files = csv_files[train_idx:val_idx]\n",
        "test_files = csv_files[val_idx:]\n",
        "\n",
        "print(f\"\\nüìä Dataset Split (from {n_files} files):\")\n",
        "print(f\"   Training:   {len(train_files)} files\")\n",
        "print(f\"   Validation: {len(val_files)} files\")\n",
        "print(f\"   Testing:    {len(test_files)} files\")\n",
        "\n",
        "# ==========================================================\n",
        "# üè∑Ô∏è FIT PREPROCESSING (SCAN ALL TRAINING FILES FOR LABELS)\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üè∑Ô∏è  Fitting Preprocessing - Scanning ALL Training Files...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# CRITICAL FIX: Scan ALL training files to collect ALL unique labels\n",
        "all_labels = set()\n",
        "sample_data = []\n",
        "\n",
        "print(f\"Scanning {len(train_files)} training files for all unique labels...\")\n",
        "for i, filepath in enumerate(train_files):\n",
        "    try:\n",
        "        # Read only the label column to save memory\n",
        "        df = pd.read_csv(filepath, low_memory=False)\n",
        "        label_col = \"Label\" if \"Label\" in df.columns else df.columns[-1]\n",
        "\n",
        "        # Collect all unique labels from this file\n",
        "        unique_labels = df[label_col].dropna().astype(str).unique()\n",
        "        all_labels.update(unique_labels)\n",
        "\n",
        "        print(f\"  File {i+1}/{len(train_files)}: {os.path.basename(filepath)} - Found {len(unique_labels)} unique labels (Total: {len(all_labels)})\")\n",
        "\n",
        "        # Sample features from first 10 files only\n",
        "        if i < 10:\n",
        "            df_sample = df.head(1000).dropna()\n",
        "            X = df_sample.drop(columns=[label_col])\n",
        "\n",
        "            # Encode objects\n",
        "            for col in X.select_dtypes(include=[\"object\"]).columns:\n",
        "                try:\n",
        "                    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "                except:\n",
        "                    X[col] = 0\n",
        "\n",
        "            sample_data.append(X.values.astype(np.float32))\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Error reading {os.path.basename(filepath)}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Convert set to sorted list for consistent encoding\n",
        "all_labels = sorted(list(all_labels))\n",
        "\n",
        "# Fit label encoder with ALL labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "n_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"\\n‚úÖ LabelEncoder fitted with {n_classes} classes\")\n",
        "print(f\"   Classes found: {', '.join(label_encoder.classes_[:10])}{'...' if n_classes > 10 else ''}\")\n",
        "\n",
        "# Fit scaler\n",
        "scaler = StandardScaler()\n",
        "for data in sample_data:\n",
        "    scaler.partial_fit(data)\n",
        "\n",
        "print(f\"‚úÖ Scaler fitted on {len(sample_data)} file samples\")\n",
        "\n",
        "# Fit PCA\n",
        "n_features = sample_data[0].shape[1]\n",
        "n_components = min(30, n_features)\n",
        "\n",
        "pca = IncrementalPCA(n_components=n_components)\n",
        "for data in sample_data:\n",
        "    X_scaled = scaler.transform(data)\n",
        "    pca.partial_fit(X_scaled)\n",
        "\n",
        "print(f\"‚úÖ PCA fitted with {n_components} components (from {n_features} features)\")\n",
        "\n",
        "del all_labels, sample_data\n",
        "clear_memory()\n",
        "print_memory_stats()\n",
        "\n",
        "# ==========================================================\n",
        "# üéì STAGE 1: TRAIN TEACHER MODEL\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéì STAGE 1: Training Teacher Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize teacher model with REDUCED sizes\n",
        "teacher_model = TeacherLSTM(\n",
        "    input_size=n_components,\n",
        "    hidden_sizes=[128, 64],  # Reduced from [256, 128, 64]\n",
        "    num_classes=n_classes,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "print(f\"\\nüèóÔ∏è  Teacher Model: {teacher_params:,} parameters\")\n",
        "print(f\"   Architecture: Input({n_components}) ‚Üí LSTM(128) ‚Üí LSTM(64) ‚Üí FC(64) ‚Üí FC(32) ‚Üí Output({n_classes})\")\n",
        "\n",
        "# Optimizer and criterion\n",
        "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
        "teacher_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training settings\n",
        "epochs_teacher = 3  # Train over all files 3 times\n",
        "batch_size = 512  # Large batch size allowed\n",
        "files_per_epoch = 20  # Process 20 files per epoch (will cycle through all 101 training files)\n",
        "\n",
        "best_teacher_acc = 0\n",
        "patience_counter = 0\n",
        "patience = 5  # Increased patience\n",
        "\n",
        "print(\"\\nüöÄ Training Teacher Model...\")\n",
        "print(f\"   Batch Size: {batch_size}\")\n",
        "print(f\"   Files per Epoch Cycle: {files_per_epoch}\")\n",
        "print(f\"   Total Training Files: {len(train_files)}\")\n",
        "print(f\"   Epochs: {epochs_teacher}\")\n",
        "\n",
        "for epoch in range(epochs_teacher):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEACHER EPOCH {epoch+1}/{epochs_teacher}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Select rotating files\n",
        "    start_idx = (epoch * files_per_epoch) % len(train_files)\n",
        "    end_idx = min(start_idx + files_per_epoch, len(train_files))\n",
        "    selected_files = train_files[start_idx:end_idx]\n",
        "\n",
        "    if len(selected_files) < files_per_epoch and len(train_files) > files_per_epoch:\n",
        "        remaining = files_per_epoch - len(selected_files)\n",
        "        selected_files += train_files[:remaining]\n",
        "\n",
        "    print(f\"Training on {len(selected_files)} files (indices {start_idx} to {end_idx})\")\n",
        "\n",
        "    # Train on each file\n",
        "    epoch_losses = []\n",
        "    for i, filepath in enumerate(selected_files):\n",
        "        print(f\"\\n  üìÇ File {i+1}/{len(selected_files)}: {os.path.basename(filepath)}\")\n",
        "\n",
        "        train_loss = train_on_file(\n",
        "            teacher_model, filepath, scaler, pca, label_encoder,\n",
        "            teacher_optimizer, teacher_criterion, device, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        print(f\"     Loss: {train_loss:.4f}\")\n",
        "        print_memory_stats()\n",
        "\n",
        "    avg_train_loss = np.mean(epoch_losses)\n",
        "\n",
        "    # Validate on subset of validation files\n",
        "    print(f\"\\n  üìä Validating...\")\n",
        "    val_loss, val_acc = evaluate_on_files(\n",
        "        teacher_model, val_files[:5], scaler, pca, label_encoder,\n",
        "        teacher_criterion, device, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  üìà Epoch Summary:\")\n",
        "    print(f\"     Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"     Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"     Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_teacher_acc:\n",
        "        best_teacher_acc = val_acc\n",
        "        torch.save(teacher_model.state_dict(), 'teacher_model.pth')\n",
        "        print(f\"  ‚úÖ Best teacher model saved! Val Acc: {val_acc:.4f}\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "print(\"\\n‚úÖ Teacher Model Training Complete!\")\n",
        "print(f\"   Best Validation Accuracy: {best_teacher_acc:.4f}\")\n",
        "\n",
        "# Load best model if it was saved, otherwise keep current\n",
        "if os.path.exists('teacher_model.pth'):\n",
        "    teacher_model.load_state_dict(torch.load('teacher_model.pth'))\n",
        "    print(\"   Loaded best teacher model from disk\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No saved model found, using final epoch weights\")\n",
        "\n",
        "# ==========================================================\n",
        "# üéí STAGE 2: KNOWLEDGE DISTILLATION - TRAIN STUDENT\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéí STAGE 2: Knowledge Distillation - Training Student Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize student model with REDUCED size\n",
        "student_model = StudentLSTM(\n",
        "    input_size=n_components,\n",
        "    hidden_size=32,  # Single layer, reduced from [32, 16]\n",
        "    num_classes=n_classes,\n",
        "    dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "reduction_ratio = teacher_params / student_params\n",
        "\n",
        "print(f\"\\nüèóÔ∏è  Student Model: {student_params:,} parameters\")\n",
        "print(f\"   Architecture: Input({n_components}) ‚Üí LSTM(32) ‚Üí FC(32) ‚Üí Output({n_classes})\")\n",
        "print(f\"\\nüìä Model Comparison:\")\n",
        "print(f\"   Teacher Parameters: {teacher_params:,}\")\n",
        "print(f\"   Student Parameters: {student_params:,}\")\n",
        "print(f\"   Size Reduction:     {reduction_ratio:.1f}x smaller\")\n",
        "\n",
        "# Optimizer and distillation loss\n",
        "student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "distillation_criterion = DistillationLoss(temperature=4.0, alpha=0.7)\n",
        "\n",
        "epochs_student = 4\n",
        "best_student_acc = 0\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"\\nüöÄ Training Student with Knowledge Distillation...\")\n",
        "print(f\"   Temperature: {distillation_criterion.temperature}\")\n",
        "print(f\"   Alpha (soft target weight): {distillation_criterion.alpha}\")\n",
        "print(f\"   Batch Size: {batch_size}\")\n",
        "print(f\"   Files per Epoch: {files_per_epoch}\")\n",
        "\n",
        "for epoch in range(epochs_student):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"STUDENT EPOCH {epoch+1}/{epochs_student}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Select rotating files\n",
        "    start_idx = (epoch * files_per_epoch) % len(train_files)\n",
        "    end_idx = min(start_idx + files_per_epoch, len(train_files))\n",
        "    selected_files = train_files[start_idx:end_idx]\n",
        "\n",
        "    if len(selected_files) < files_per_epoch and len(train_files) > files_per_epoch:\n",
        "        remaining = files_per_epoch - len(selected_files)\n",
        "        selected_files += train_files[:remaining]\n",
        "\n",
        "    print(f\"Training on {len(selected_files)} files (indices {start_idx} to {end_idx})\")\n",
        "\n",
        "    # Train with distillation\n",
        "    epoch_losses = []\n",
        "    for i, filepath in enumerate(selected_files):\n",
        "        print(f\"\\n  üìÇ File {i+1}/{len(selected_files)}: {os.path.basename(filepath)}\")\n",
        "\n",
        "        train_loss = train_on_file(\n",
        "            student_model, filepath, scaler, pca, label_encoder,\n",
        "            student_optimizer, distillation_criterion, device,\n",
        "            batch_size=batch_size, is_distillation=True, teacher_model=teacher_model\n",
        "        )\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        print(f\"     Loss: {train_loss:.4f}\")\n",
        "        print_memory_stats()\n",
        "\n",
        "    avg_train_loss = np.mean(epoch_losses)\n",
        "\n",
        "    # Validate\n",
        "    print(f\"\\n  üìä Validating...\")\n",
        "    val_criterion = nn.CrossEntropyLoss()\n",
        "    val_loss, val_acc = evaluate_on_files(\n",
        "        student_model, val_files[:5], scaler, pca, label_encoder,\n",
        "        val_criterion, device, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  üìà Epoch Summary:\")\n",
        "    print(f\"     Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"     Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"     Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_student_acc:\n",
        "        best_student_acc = val_acc\n",
        "        torch.save(student_model.state_dict(), 'student_model.pth')\n",
        "        print(f\"  ‚úÖ Best student model saved! Val Acc: {val_acc:.4f}\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "print(\"\\n‚úÖ Student Model Training Complete!\")\n",
        "print(f\"   Best Validation Accuracy: {best_student_acc:.4f}\")\n",
        "\n",
        "# Load best model if it exists\n",
        "if os.path.exists('student_model.pth'):\n",
        "    student_model.load_state_dict(torch.load('student_model.pth'))\n",
        "    print(\"   Loaded best student model from disk\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No saved model found, using final epoch weights\")\n",
        "\n",
        "# ==========================================================\n",
        "# üìà STAGE 3: FINAL EVALUATION\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà STAGE 3: Final Evaluation on Test Set\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def evaluate_model_detailed(model, model_name, file_list):\n",
        "    \"\"\"Evaluate model on test set with detailed metrics\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.eval()\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "\n",
        "    for i, filepath in enumerate(file_list):\n",
        "        print(f\"Processing file {i+1}/{len(file_list)}: {os.path.basename(filepath)}\")\n",
        "\n",
        "        X_file, y_file = load_and_process_file(filepath, scaler, pca, label_encoder)\n",
        "\n",
        "        if X_file is None:\n",
        "            continue\n",
        "\n",
        "        dataset = FullFileDataset(X_file, y_file)\n",
        "        dataloader = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in dataloader:\n",
        "                X_batch = X_batch.unsqueeze(1).to(device)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                y_true_all.extend(y_batch.numpy())\n",
        "                y_pred_all.extend(predicted.cpu().numpy())\n",
        "\n",
        "                del X_batch, outputs\n",
        "                clear_memory()\n",
        "\n",
        "        del X_file, y_file, dataset, dataloader\n",
        "        clear_memory()\n",
        "\n",
        "    y_true_all = np.array(y_true_all)\n",
        "    y_pred_all = np.array(y_pred_all)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_all, y_pred_all)\n",
        "    precision = precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\nüìä {model_name} Performance:\")\n",
        "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall:    {recall:.4f}\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    return y_true_all, y_pred_all, accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate both models\n",
        "teacher_results = evaluate_model_detailed(teacher_model, \"TEACHER MODEL\", test_files)\n",
        "student_results = evaluate_model_detailed(student_model, \"STUDENT MODEL (Distilled)\", test_files)\n",
        "\n",
        "# ==========================================================\n",
        "# üìä GENERATE REPORTS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Generating Final Report...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_true, y_pred, s_acc, s_prec, s_rec, s_f1 = student_results\n",
        "_, _, t_acc, t_prec, t_rec, t_f1 = teacher_results\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Student Model Confusion Matrix (Knowledge Distillation)', fontsize=16, pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(rotation=0, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('student_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Confusion matrix saved as 'student_confusion_matrix.png'\")\n",
        "\n",
        "# Performance comparison\n",
        "performance_retention = (s_acc / t_acc) * 100 if t_acc > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä FINAL COMPARISON: TEACHER vs STUDENT\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n{'Metric':<15} {'Teacher':<15} {'Student':<15} {'Difference':<15}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Accuracy':<15} {t_acc:<15.4f} {s_acc:<15.4f} {(s_acc-t_acc):<15.4f}\")\n",
        "print(f\"{'Precision':<15} {t_prec:<15.4f} {s_prec:<15.4f} {(s_prec-t_prec):<15.4f}\")\n",
        "print(f\"{'Recall':<15} {t_rec:<15.4f} {s_rec:<15.4f} {(s_rec-t_rec):<15.4f}\")\n",
        "print(f\"{'F1-Score':<15} {t_f1:<15.4f} {s_f1:<15.4f} {(s_f1-t_f1):<15.4f}\")\n",
        "print(f\"{'Parameters':<15} {teacher_params:<15,} {student_params:<15,} {'-':<15}\")\n",
        "print(f\"{'Model Size':<15} {'1.0x':<15} {f'{1/reduction_ratio:.2f}x':<15} {f'{reduction_ratio:.1f}x smaller':<15}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüéØ Performance Retention: {performance_retention:.2f}%\")\n",
        "print(f\"üéØ Model Size Reduction: {reduction_ratio:.1f}x smaller\")\n",
        "print(f\"üéØ Parameter Reduction: {((teacher_params - student_params) / teacher_params * 100):.1f}% fewer parameters\")\n",
        "\n",
        "# ==========================================================\n",
        "# üíæ SAVE MODELS AND PREPROCESSING OBJECTS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üíæ Saving Models and Preprocessing Objects\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save PyTorch models\n",
        "torch.save({\n",
        "    'model_state_dict': teacher_model.state_dict(),\n",
        "    'input_size': n_components,\n",
        "    'hidden_sizes': [128, 64],\n",
        "    'num_classes': n_classes,\n",
        "    'accuracy': t_acc,\n",
        "    'params': teacher_params\n",
        "}, 'teacher_model_complete.pth')\n",
        "print(\"‚úÖ Saved: teacher_model_complete.pth\")\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': student_model.state_dict(),\n",
        "    'input_size': n_components,\n",
        "    'hidden_size': 32,\n",
        "    'num_classes': n_classes,\n",
        "    'accuracy': s_acc,\n",
        "    'params': student_params\n",
        "}, 'student_model_complete.pth')\n",
        "print(\"‚úÖ Saved: student_model_complete.pth\")\n",
        "\n",
        "# Save preprocessing objects\n",
        "preprocessing_objects = {\n",
        "    'scaler': scaler,\n",
        "    'pca': pca,\n",
        "    'label_encoder': label_encoder\n",
        "}\n",
        "\n",
        "with open('preprocessing.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_objects, f)\n",
        "print(\"‚úÖ Saved: preprocessing.pkl\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'n_classes': int(n_classes),\n",
        "    'n_features': int(n_features),\n",
        "    'n_components': int(n_components),\n",
        "    'teacher_params': int(teacher_params),\n",
        "    'student_params': int(student_params),\n",
        "    'teacher_accuracy': float(t_acc),\n",
        "    'student_accuracy': float(s_acc),\n",
        "    'size_reduction': float(reduction_ratio),\n",
        "    'performance_retention': float(performance_retention),\n",
        "    'total_files': len(csv_files),\n",
        "    'train_files': len(train_files),\n",
        "    'val_files': len(val_files),\n",
        "    'test_files': len(test_files),\n",
        "    'classes': label_encoder.classes_.tolist()\n",
        "}\n",
        "\n",
        "with open('model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "print(\"‚úÖ Saved: model_metadata.json\")\n",
        "\n",
        "# Create summary\n",
        "with open('model_summary.txt', 'w') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"KNOWLEDGE DISTILLATION - PYTORCH MODEL SUMMARY\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"DATASET INFORMATION:\\n\")\n",
        "    f.write(f\"  Total Files: {len(csv_files)}\\n\")\n",
        "    f.write(f\"  Training Files: {len(train_files)}\\n\")\n",
        "    f.write(f\"  Validation Files: {len(val_files)}\\n\")\n",
        "    f.write(f\"  Test Files: {len(test_files)}\\n\\n\")\n",
        "\n",
        "    f.write(\"TEACHER MODEL:\\n\")\n",
        "    f.write(f\"  Architecture: LSTM [128, 64]\\n\")\n",
        "    f.write(f\"  Parameters: {teacher_params:,}\\n\")\n",
        "    f.write(f\"  Accuracy: {t_acc:.4f}\\n\")\n",
        "    f.write(f\"  Precision: {t_prec:.4f}\\n\")\n",
        "    f.write(f\"  Recall: {t_rec:.4f}\\n\")\n",
        "    f.write(f\"  F1-Score: {t_f1:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"STUDENT MODEL (DISTILLED):\\n\")\n",
        "    f.write(f\"  Architecture: LSTM [32]\\n\")\n",
        "    f.write(f\"  Parameters: {student_params:,}\\n\")\n",
        "    f.write(f\"  Accuracy: {s_acc:.4f}\\n\")\n",
        "    f.write(f\"  Precision: {s_prec:.4f}\\n\")\n",
        "    f.write(f\"  Recall: {s_rec:.4f}\\n\")\n",
        "    f.write(f\"  F1-Score: {s_f1:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"COMPRESSION METRICS:\\n\")\n",
        "    f.write(f\"  Size Reduction: {reduction_ratio:.1f}x smaller\\n\")\n",
        "    f.write(f\"  Performance Retention: {performance_retention:.2f}%\\n\")\n",
        "    f.write(f\"  Parameter Reduction: {((teacher_params - student_params) / teacher_params * 100):.1f}%\\n\\n\")\n",
        "\n",
        "    f.write(\"FILES GENERATED:\\n\")\n",
        "    f.write(\"  - teacher_model_complete.pth (Teacher model with metadata)\\n\")\n",
        "    f.write(\"  - student_model_complete.pth (Student model with metadata)\\n\")\n",
        "    f.write(\"  - preprocessing.pkl (Scaler, PCA, Label Encoder)\\n\")\n",
        "    f.write(\"  - model_metadata.json (Model specifications)\\n\")\n",
        "    f.write(\"  - student_confusion_matrix.png (Confusion matrix visualization)\\n\")\n",
        "\n",
        "print(\"‚úÖ Saved: model_summary.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ KNOWLEDGE DISTILLATION COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n‚ú® Successfully processed all {len(csv_files)} files!\")\n",
        "print(f\"‚ú® Teacher Model: {teacher_params:,} parameters ‚Üí Accuracy: {t_acc:.4f}\")\n",
        "print(f\"‚ú® Student Model: {student_params:,} parameters ‚Üí Accuracy: {s_acc:.4f}\")\n",
        "print(f\"‚ú® Compression: {reduction_ratio:.1f}x smaller with {performance_retention:.1f}% performance retention\")\n",
        "print(\"\\nüì¶ All models saved and ready for deployment!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "sF3aPhEPfgNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f3e3cb-b3fc-48a3-8132-df0a1174e684"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üéÆ GPU Configuration\n",
            "================================================================================\n",
            "‚úÖ GPU detected: Tesla T4\n",
            "‚úÖ CUDA Version: 12.6\n",
            "‚úÖ GPU Memory: 15.83 GB\n",
            "‚úÖ cuDNN autotuner enabled\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üì• Downloading CIC-IoT-2023 Dataset from Kaggle...\n",
            "================================================================================\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/akashdogra/cic-iot-2023?dataset_version_number=1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.77G/2.77G [00:38<00:00, 77.7MB/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset downloaded to: /root/.cache/kagglehub/datasets/akashdogra/cic-iot-2023/versions/1\n",
            "üìÇ Found 169 CSV files.\n",
            "\n",
            "üìä Dataset Split (from 169 files):\n",
            "   Training:   101 files\n",
            "   Validation: 34 files\n",
            "   Testing:    34 files\n",
            "\n",
            "================================================================================\n",
            "üè∑Ô∏è  Fitting Preprocessing - Scanning ALL Training Files...\n",
            "================================================================================\n",
            "Scanning 101 training files for all unique labels...\n",
            "  File 1/101: part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 2/101: part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 3/101: part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 4/101: part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 5/101: part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 6/101: part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 7/101: part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 8/101: part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 9/101: part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 10/101: part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 11/101: part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 12/101: part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 13/101: part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 14/101: part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 15/101: part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 16/101: part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 17/101: part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 18/101: part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 19/101: part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 20/101: part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 21/101: part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 22/101: part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 23/101: part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 24/101: part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 25/101: part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 26/101: part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 27/101: part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 28/101: part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 29/101: part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 30/101: part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 31/101: part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 32/101: part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 33/101: part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 34/101: part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 35/101: part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 36/101: part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 37/101: part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 38/101: part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 39/101: part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 40/101: part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 41/101: part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 42/101: part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 43/101: part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 44/101: part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 45/101: part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 46/101: part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 47/101: part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 48/101: part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 49/101: part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 50/101: part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 51/101: part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 52/101: part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 53/101: part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 54/101: part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 55/101: part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 56/101: part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 57/101: part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 58/101: part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 59/101: part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 60/101: part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 61/101: part-00060-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 62/101: part-00061-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 63/101: part-00062-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 64/101: part-00063-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 65/101: part-00064-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 66/101: part-00065-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 67/101: part-00066-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 68/101: part-00067-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 69/101: part-00068-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 70/101: part-00069-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 71/101: part-00070-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 72/101: part-00071-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 73/101: part-00072-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 74/101: part-00073-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 75/101: part-00074-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 76/101: part-00075-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 77/101: part-00076-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 78/101: part-00077-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 79/101: part-00078-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 80/101: part-00079-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 81/101: part-00080-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 82/101: part-00081-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 83/101: part-00082-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 84/101: part-00083-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 85/101: part-00084-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 86/101: part-00085-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 87/101: part-00086-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 88/101: part-00087-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 89/101: part-00088-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 90/101: part-00089-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 91/101: part-00090-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 92/101: part-00091-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 93/101: part-00092-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 94/101: part-00093-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 95/101: part-00094-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 96/101: part-00095-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 97/101: part-00096-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 98/101: part-00097-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 99/101: part-00098-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 100/101: part-00099-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "  File 101/101: part-00100-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv - Found 34 unique labels (Total: 34)\n",
            "\n",
            "‚úÖ LabelEncoder fitted with 34 classes\n",
            "   Classes found: Backdoor_Malware, BenignTraffic, BrowserHijacking, CommandInjection, DDoS-ACK_Fragmentation, DDoS-HTTP_Flood, DDoS-ICMP_Flood, DDoS-ICMP_Fragmentation, DDoS-PSHACK_Flood, DDoS-RSTFINFlood...\n",
            "‚úÖ Scaler fitted on 10 file samples\n",
            "‚úÖ PCA fitted with 30 components (from 46 features)\n",
            "üíæ RAM Usage: 0.64 GB | GPU: 0.00 GB\n",
            "\n",
            "================================================================================\n",
            "üéì STAGE 1: Training Teacher Model\n",
            "================================================================================\n",
            "\n",
            "üèóÔ∏è  Teacher Model: 138,946 parameters\n",
            "   Architecture: Input(30) ‚Üí LSTM(128) ‚Üí LSTM(64) ‚Üí FC(64) ‚Üí FC(32) ‚Üí Output(34)\n",
            "\n",
            "üöÄ Training Teacher Model...\n",
            "   Batch Size: 512\n",
            "   Files per Epoch Cycle: 20\n",
            "   Total Training Files: 101\n",
            "   Epochs: 3\n",
            "\n",
            "================================================================================\n",
            "TEACHER EPOCH 1/3\n",
            "================================================================================\n",
            "Training on 20 files (indices 0 to 20)\n",
            "\n",
            "  üìÇ File 1/20: part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.0492\n",
            "üíæ RAM Usage: 1.16 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4879\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4643\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4493\n",
            "üíæ RAM Usage: 1.18 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4435\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4358\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4260\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4192\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4114\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.4074\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3998\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3911\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3823\n",
            "üíæ RAM Usage: 1.19 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3727\n",
            "üíæ RAM Usage: 1.26 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3645\n",
            "üíæ RAM Usage: 1.23 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3576\n",
            "üíæ RAM Usage: 1.20 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3513\n",
            "üíæ RAM Usage: 1.20 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3448\n",
            "üíæ RAM Usage: 1.25 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3391\n",
            "üíæ RAM Usage: 1.20 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3321\n",
            "üíæ RAM Usage: 1.20 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.4315\n",
            "     Val Loss: 0.3067\n",
            "     Val Accuracy: 0.8682\n",
            "  ‚úÖ Best teacher model saved! Val Acc: 0.8682\n",
            "\n",
            "================================================================================\n",
            "TEACHER EPOCH 2/3\n",
            "================================================================================\n",
            "Training on 20 files (indices 20 to 40)\n",
            "\n",
            "  üìÇ File 1/20: part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3258\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3189\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3151\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.3071\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2946\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2862\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2783\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2735\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2702\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2630\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2619\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2561\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2512\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2466\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2473\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2373\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2255\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2171\n",
            "üíæ RAM Usage: 1.30 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.2113\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1960\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.2641\n",
            "     Val Loss: 0.1550\n",
            "     Val Accuracy: 0.9321\n",
            "  ‚úÖ Best teacher model saved! Val Acc: 0.9321\n",
            "\n",
            "================================================================================\n",
            "TEACHER EPOCH 3/3\n",
            "================================================================================\n",
            "Training on 20 files (indices 40 to 60)\n",
            "\n",
            "  üìÇ File 1/20: part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1900\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1841\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1807\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1793\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1935\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1711\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1769\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1765\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1794\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1713\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1743\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1656\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1677\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1734\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1830\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1556\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1699\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1629\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1579\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.1637\n",
            "üíæ RAM Usage: 1.31 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.1739\n",
            "     Val Loss: 0.1127\n",
            "     Val Accuracy: 0.9611\n",
            "  ‚úÖ Best teacher model saved! Val Acc: 0.9611\n",
            "\n",
            "‚úÖ Teacher Model Training Complete!\n",
            "   Best Validation Accuracy: 0.9611\n",
            "   Loaded best teacher model from disk\n",
            "\n",
            "================================================================================\n",
            "üéí STAGE 2: Knowledge Distillation - Training Student Model\n",
            "================================================================================\n",
            "\n",
            "üèóÔ∏è  Student Model: 10,370 parameters\n",
            "   Architecture: Input(30) ‚Üí LSTM(32) ‚Üí FC(32) ‚Üí Output(34)\n",
            "\n",
            "üìä Model Comparison:\n",
            "   Teacher Parameters: 138,946\n",
            "   Student Parameters: 10,370\n",
            "   Size Reduction:     13.4x smaller\n",
            "\n",
            "üöÄ Training Student with Knowledge Distillation...\n",
            "   Temperature: 4.0\n",
            "   Alpha (soft target weight): 0.7\n",
            "   Batch Size: 512\n",
            "   Files per Epoch: 20\n",
            "\n",
            "================================================================================\n",
            "STUDENT EPOCH 1/4\n",
            "================================================================================\n",
            "Training on 20 files (indices 0 to 20)\n",
            "\n",
            "  üìÇ File 1/20: part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 12.0894\n",
            "üíæ RAM Usage: 1.36 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 2.6724\n",
            "üíæ RAM Usage: 1.37 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.8982\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.6120\n",
            "üíæ RAM Usage: 1.38 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.4894\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.4052\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.3456\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.3000\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.2605\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.2338\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.1994\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.1752\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.1543\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.1284\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.0945\n",
            "üíæ RAM Usage: 1.38 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.0559\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 1.0180\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.9865\n",
            "üíæ RAM Usage: 1.38 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.9558\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.9267\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 1.8501\n",
            "     Val Loss: 0.4064\n",
            "     Val Accuracy: 0.8489\n",
            "  ‚úÖ Best student model saved! Val Acc: 0.8489\n",
            "\n",
            "================================================================================\n",
            "STUDENT EPOCH 2/4\n",
            "================================================================================\n",
            "Training on 20 files (indices 20 to 40)\n",
            "\n",
            "  üìÇ File 1/20: part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.9102\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8891\n",
            "üíæ RAM Usage: 1.38 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8784\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8586\n",
            "üíæ RAM Usage: 1.40 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8505\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8323\n",
            "üíæ RAM Usage: 1.41 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8169\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8098\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.8006\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7923\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7815\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7739\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7729\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7612\n",
            "üíæ RAM Usage: 1.41 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7571\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7518\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7461\n",
            "üíæ RAM Usage: 1.41 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7349\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7353\n",
            "üíæ RAM Usage: 1.39 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7249\n",
            "üíæ RAM Usage: 1.38 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.7989\n",
            "     Val Loss: 0.3555\n",
            "     Val Accuracy: 0.8657\n",
            "  ‚úÖ Best student model saved! Val Acc: 0.8657\n",
            "\n",
            "================================================================================\n",
            "STUDENT EPOCH 3/4\n",
            "================================================================================\n",
            "Training on 20 files (indices 40 to 60)\n",
            "\n",
            "  üìÇ File 1/20: part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7172\n",
            "üíæ RAM Usage: 1.36 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7088\n",
            "üíæ RAM Usage: 1.36 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7016\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.7008\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6931\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6835\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6814\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6718\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6717\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6624\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6595\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6519\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6457\n",
            "üíæ RAM Usage: 1.36 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6393\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6352\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6289\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6268\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6183\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6126\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6073\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.6609\n",
            "     Val Loss: 0.3115\n",
            "     Val Accuracy: 0.8817\n",
            "  ‚úÖ Best student model saved! Val Acc: 0.8817\n",
            "\n",
            "================================================================================\n",
            "STUDENT EPOCH 4/4\n",
            "================================================================================\n",
            "Training on 20 files (indices 60 to 80)\n",
            "\n",
            "  üìÇ File 1/20: part-00060-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6077\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 2/20: part-00061-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.6014\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 3/20: part-00062-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5965\n",
            "üíæ RAM Usage: 1.35 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 4/20: part-00063-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5985\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 5/20: part-00064-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5890\n",
            "üíæ RAM Usage: 1.34 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 6/20: part-00065-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5871\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 7/20: part-00066-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5819\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 8/20: part-00067-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5835\n",
            "üíæ RAM Usage: 1.33 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 9/20: part-00068-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5743\n",
            "üíæ RAM Usage: 1.32 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 10/20: part-00069-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5765\n",
            "üíæ RAM Usage: 1.30 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 11/20: part-00070-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5724\n",
            "üíæ RAM Usage: 1.27 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 12/20: part-00071-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5651\n",
            "üíæ RAM Usage: 1.26 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 13/20: part-00072-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5667\n",
            "üíæ RAM Usage: 1.27 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 14/20: part-00073-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5626\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 15/20: part-00074-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5590\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 16/20: part-00075-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5546\n",
            "üíæ RAM Usage: 1.25 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 17/20: part-00076-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5481\n",
            "üíæ RAM Usage: 1.26 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 18/20: part-00077-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5430\n",
            "üíæ RAM Usage: 1.25 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 19/20: part-00078-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5340\n",
            "üíæ RAM Usage: 1.25 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìÇ File 20/20: part-00079-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "     Loss: 0.5282\n",
            "üíæ RAM Usage: 1.24 GB | GPU: 0.02 GB\n",
            "\n",
            "  üìä Validating...\n",
            "\n",
            "  üìà Epoch Summary:\n",
            "     Avg Train Loss: 0.5715\n",
            "     Val Loss: 0.2710\n",
            "     Val Accuracy: 0.8829\n",
            "  ‚úÖ Best student model saved! Val Acc: 0.8829\n",
            "\n",
            "‚úÖ Student Model Training Complete!\n",
            "   Best Validation Accuracy: 0.8829\n",
            "   Loaded best student model from disk\n",
            "\n",
            "================================================================================\n",
            "üìà STAGE 3: Final Evaluation on Test Set\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating TEACHER MODEL...\n",
            "============================================================\n",
            "Processing file 1/34: part-00135-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 2/34: part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 3/34: part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 4/34: part-00138-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 5/34: part-00139-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 6/34: part-00140-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 7/34: part-00141-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 8/34: part-00142-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 9/34: part-00143-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 10/34: part-00144-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 11/34: part-00145-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 12/34: part-00146-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 13/34: part-00147-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 14/34: part-00148-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 15/34: part-00149-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 16/34: part-00150-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 17/34: part-00151-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 18/34: part-00152-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 19/34: part-00153-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 20/34: part-00154-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 21/34: part-00155-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 22/34: part-00156-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 23/34: part-00157-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 24/34: part-00158-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 25/34: part-00159-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 26/34: part-00160-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 27/34: part-00161-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 28/34: part-00162-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 29/34: part-00163-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 30/34: part-00164-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 31/34: part-00165-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 32/34: part-00166-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 33/34: part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 34/34: part-00168-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "\n",
            "üìä TEACHER MODEL Performance:\n",
            "   Accuracy:  0.9611\n",
            "   Precision: 0.9610\n",
            "   Recall:    0.9611\n",
            "   F1-Score:  0.9598\n",
            "\n",
            "============================================================\n",
            "Evaluating STUDENT MODEL (Distilled)...\n",
            "============================================================\n",
            "Processing file 1/34: part-00135-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 2/34: part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 3/34: part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 4/34: part-00138-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 5/34: part-00139-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 6/34: part-00140-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 7/34: part-00141-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 8/34: part-00142-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 9/34: part-00143-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 10/34: part-00144-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 11/34: part-00145-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 12/34: part-00146-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 13/34: part-00147-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 14/34: part-00148-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 15/34: part-00149-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
            "Processing file 16/34: part-00150-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNIzUwhihpu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}